{
 "cells": [
  {
   "cell_type": "code",
   "id": "1983646a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:24:51.594681Z",
     "start_time": "2025-12-01T21:24:51.584677Z"
    }
   },
   "source": "# !pip install -r requirements.txt",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:05.437666Z",
     "start_time": "2025-12-01T21:24:51.604676Z"
    }
   },
   "source": [
    "from platform import python_version\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "from typing import List, Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:05.670657Z",
     "start_time": "2025-12-01T21:25:05.662652Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c40fd91bf2b1b859",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:05.685439Z",
     "start_time": "2025-12-01T21:25:05.680505Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "15db202954cf6949",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### альтернативный вариант",
   "id": "59c443aabc6aead7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:06.648353Z",
     "start_time": "2025-12-01T21:25:05.694464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from platform import python_version\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "# (если понадобится) from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n"
   ],
   "id": "15a3ece66ad1a625",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:06.727051Z",
     "start_time": "2025-12-01T21:25:06.716966Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5d51ed2cb33f6ce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:08.033256Z",
     "start_time": "2025-12-01T21:25:06.736078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2.1 Настройка подключения к Qdrant\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "client_qdrant = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "# 2.2 Настройка подключения к ClickHouse\n",
    "CH_HOST = '84.201.160.255'   # или из окружения\n",
    "CH_PORT = 8123\n",
    "CLICKHOUSE_USER = 'peter'\n",
    "CLICKHOUSE_PASSWORD = '1234'\n",
    "\n",
    "client_clickhouse = clickhouse_connect.get_client(\n",
    "    host=CH_HOST,\n",
    "    port=CH_PORT,\n",
    "    username=CLICKHOUSE_USER,\n",
    "    password=CLICKHOUSE_PASSWORD\n",
    ")\n"
   ],
   "id": "ec6a64073f700f90",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:09.349935Z",
     "start_time": "2025-12-01T21:25:08.120252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emails_df = client_clickhouse.query_df(\"\"\"\n",
    "    SELECT id, message_id, subject, from_addr, to_addr, cc_addr, bcc_addr,\n",
    "           sent_at_utc, folder, body_text, body_html\n",
    "    FROM mailkb.emails\n",
    "    ORDER BY sent_at_utc DESC, message_id DESC\n",
    "    LIMIT 100\n",
    "\"\"\")\n"
   ],
   "id": "8f0fea8d93ad96f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Настройка коллекции в Qdrant (с вариантами)",
   "id": "cd4a12c20dc22a57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:10.016350Z",
     "start_time": "2025-12-01T21:25:09.467975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_URL = \"http://localhost:8000/v1\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    # model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    model=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    api_key=\"not-needed\",\n",
    "    base_url=BASE_URL,\n",
    "    tiktoken_enabled=False,\n",
    ")"
   ],
   "id": "717f4acf107f5099",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:15.855483Z",
     "start_time": "2025-12-01T21:25:10.029320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# тест - получаем размерность эмбединга\n",
    "vec = embeddings.embed_query(\"test\")\n",
    "EMBEDDING_DIM = len(vec)\n",
    "EMBEDDING_DIM"
   ],
   "id": "db8d065b41469ee2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:15.994070Z",
     "start_time": "2025-12-01T21:25:15.969974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "COLLECTION_NAME = \"mailkb_emails\"\n",
    "\n",
    "\n",
    "# ---------- Вариант A: УДАЛИТЬ коллекцию и СОЗДАТЬ ЗАНОВО ----------\n",
    "def setup_collection_recreate():\n",
    "    \"\"\"\n",
    "    Полная пересоздача коллекции: удаляем старую, создаём новую с нужной размерностью.\n",
    "    Полезно, когда меняешь модель / размер эмбеддинга.\n",
    "    \"\"\"\n",
    "    client_qdrant.recreate_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=EMBEDDING_DIM,\n",
    "            distance=Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    qv = QdrantVectorStore(\n",
    "        client=client_qdrant,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        # если не хочешь, чтобы он дополнительно валидировал конфиг:\n",
    "        # validate_collection_config=False,\n",
    "    )\n",
    "    return qv\n",
    "\n",
    "\n",
    "# ---------- Вариант B: СОЗДАТЬ, если коллекции ещё нет ----------\n",
    "def setup_collection_create_if_not_exists():\n",
    "    \"\"\"\n",
    "    Если коллекции нет — создаём.\n",
    "    Если есть — НЕ трогаем (данные сохраняются).\n",
    "    \"\"\"\n",
    "    collections = client_qdrant.get_collections().collections\n",
    "    existing_names = {c.name for c in collections}\n",
    "\n",
    "    if COLLECTION_NAME not in existing_names:\n",
    "        client_qdrant.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(\n",
    "                size=EMBEDDING_DIM,\n",
    "                distance=Distance.COSINE,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    qv = QdrantVectorStore(\n",
    "        client=client_qdrant,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        # при желании можно отключить проверку:\n",
    "        # validate_collection_config=False,\n",
    "    )\n",
    "    return qv\n",
    "\n",
    "\n",
    "# ---------- Вариант C: ИСПОЛЬЗОВАТЬ ТЕКУЩУЮ коллекцию как есть ----------\n",
    "def setup_collection_use_existing():\n",
    "    \"\"\"\n",
    "    Просто подключаемся к уже существующей коллекции.\n",
    "    Ничего не создаём и не удаляем.\n",
    "    Важно: размерность в Qdrant должна совпадать с размерностью эмбеддинга.\n",
    "    \"\"\"\n",
    "    qv = QdrantVectorStore(\n",
    "        client=client_qdrant,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        # если вдруг Qdrant создан с \"левым\" размером, и ты осознанно хочешь\n",
    "        # отключить проверку (НЕ рекомендую в бою):\n",
    "        # validate_collection_config=False,\n",
    "    )\n",
    "    return qv\n",
    "\n"
   ],
   "id": "2cccb2d9b7aad411",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:17.246783Z",
     "start_time": "2025-12-01T21:25:16.003062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Пример использования одного из вариантов:\n",
    "# qv = setup_collection_recreate()\n",
    "qv = setup_collection_create_if_not_exists()\n",
    "# qv = setup_collection_use_existing()"
   ],
   "id": "403d3d548ee09dee",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подготовка утилит (нормализация, участники, превращение в документы)",
   "id": "3ea2062e5553ca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:17.371914Z",
     "start_time": "2025-12-01T21:25:17.346919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RE_PREFIX = re.compile(r'^\\s*(re|fw|fwd):\\s*', flags=re.IGNORECASE)\n",
    "\n",
    "def normalize_subject(subj: str) -> str:\n",
    "    s = subj or \"\"\n",
    "    while True:\n",
    "        ns = RE_PREFIX.sub('', s).strip()\n",
    "        if ns == s:\n",
    "            break\n",
    "        s = ns\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s.lower()\n",
    "\n",
    "def participants_list(row) -> list[str]:\n",
    "    def _norm(x):\n",
    "        if not x:\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return [str(i).strip() for i in x if str(i).strip()]\n",
    "        return [p.strip() for p in re.split(r'[;,]', str(x)) if p.strip()]\n",
    "    people = _norm(row.get(\"from_addr\")) + _norm(row.get(\"to_addr\")) \\\n",
    "           + _norm(row.get(\"cc_addr\")) + _norm(row.get(\"bcc_addr\"))\n",
    "    return sorted(set(people))\n",
    "\n",
    "def build_docs(df: pd.DataFrame) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "    for _, r in df.iterrows():\n",
    "        subj = (r.get(\"subject\") or \"\").strip()\n",
    "        body = (r.get(\"body_text\") or \"\").strip()\n",
    "        if not body:\n",
    "            body = (r.get(\"body_html\") or \"\").strip()\n",
    "        text = (subj + \"\\n\\n\" + body).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        parts = participants_list(r)\n",
    "        norm_subj = normalize_subject(subj)\n",
    "        thread_key = f\"{norm_subj}||{';'.join(sorted(parts))}\"\n",
    "        meta = {\n",
    "            \"row_id\": r.get(\"id\"),\n",
    "            \"message_id\": r.get(\"message_id\"),\n",
    "            \"subject\": subj,\n",
    "            \"sent_at_utc\": str(r.get(\"sent_at_utc\")),\n",
    "            \"folder\": r.get(\"folder\"),\n",
    "            \"from_addr\": parts[:1],\n",
    "            \"participants\": parts,\n",
    "            \"thread_key\": thread_key,\n",
    "        }\n",
    "        docs.append(Document(page_content=text, metadata=meta))\n",
    "    return docs\n"
   ],
   "id": "89425726b940b689",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Очистка текста и разбивка на чанки",
   "id": "a864419d723b545c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:17.402647Z",
     "start_time": "2025-12-01T21:25:17.380917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)\n",
    "\n",
    "RE_QUOTED = re.compile(r\"(?m)^(>+).*$\")  # строки, начинающиеся с >\n",
    "RE_HDR = re.compile(r\"(?:^|\\n)(from:|sent:|to:|subject:).*(?:\\n.*){0,20}\", re.IGNORECASE)\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\")\n",
    "    t = RE_HDR.sub(\"\\n\", t)\n",
    "    t = RE_QUOTED.sub(\"\", t)\n",
    "    t = \"\\n\".join([ln.strip() for ln in t.split(\"\\n\") if ln.strip()])\n",
    "    return t\n",
    "\n",
    "def preprocess_and_chunk(docs: list[Document]) -> list[Document]:\n",
    "    cleaned: list[Document] = []\n",
    "    for d in docs:\n",
    "        txt = clean_text(d.page_content)\n",
    "        if not txt:\n",
    "            continue\n",
    "        cleaned.append(Document(page_content=txt, metadata=d.metadata))\n",
    "    return splitter.split_documents(cleaned)\n"
   ],
   "id": "6742666df770cc84",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Генерация стабильных ID и загрузка в Qdrant",
   "id": "9fe224749ac78481"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:17.433079Z",
     "start_time": "2025-12-01T21:25:17.410598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_ids(chunks: list[Document]) -> list[str]:\n",
    "    counters: dict[str, int] = {}\n",
    "    ids: list[str] = []\n",
    "    for c in chunks:\n",
    "        msg = c.metadata.get(\"message_id\") or c.metadata.get(\"row_id\") or \"noid\"\n",
    "        i = counters.get(msg, 0)\n",
    "        counters[msg] = i + 1\n",
    "        raw = f\"{msg}::chunk_{i}\"\n",
    "        uid = uuid.uuid5(uuid.NAMESPACE_URL, raw)\n",
    "        ids.append(str(uid))\n",
    "    return ids\n",
    "\n",
    "def process_batch(df: pd.DataFrame):\n",
    "    docs = build_docs(df)\n",
    "    chunks = preprocess_and_chunk(docs)\n",
    "    if not chunks:\n",
    "        return\n",
    "    ids = make_ids(chunks)\n",
    "    texts = [c.page_content for c in chunks]\n",
    "    metadatas = [c.metadata for c in chunks]\n",
    "    qv.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n"
   ],
   "id": "d7bfa92f1e4ebab8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запуск загрузки (батчинг)",
   "id": "8ee07527a2cc0939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:17.448589Z",
     "start_time": "2025-12-01T21:25:17.442525Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c447051b0c166a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:31.204515Z",
     "start_time": "2025-12-01T21:25:17.457024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Пример: полный цикл с батчингом\n",
    "# BATCH_SIZE = 5000\n",
    "# offset = 0\n",
    "#\n",
    "# while True:\n",
    "#     df_batch = client_clickhouse.query_df(f\"\"\"\n",
    "#         SELECT id, message_id, subject, from_addr, to_addr, cc_addr, bcc_addr,\n",
    "#                sent_at_utc, folder, body_text, body_html\n",
    "#         FROM mailkb.emails\n",
    "#         ORDER BY sent_at_utc ASC\n",
    "#         LIMIT {BATCH_SIZE} OFFSET {offset}\n",
    "#     \"\"\")\n",
    "#     if df_batch.empty:\n",
    "#         break\n",
    "#     process_batch(df_batch)\n",
    "#     offset += len(df_batch)\n",
    "#     print(f\"Processed {offset} rows\")\n",
    "\n",
    "# Или — одноразовая загрузка небольшой выборки\n",
    "offset = 0\n",
    "df_small = client_clickhouse.query_df(\"\"\"\n",
    "    SELECT id, message_id, subject, from_addr, to_addr, cc_addr, bcc_addr,\n",
    "           sent_at_utc, folder, body_text, body_html\n",
    "    FROM mailkb.emails\n",
    "    ORDER BY sent_at_utc DESC, message_id DESC\n",
    "    LIMIT 100\n",
    "\"\"\")\n",
    "process_batch(df_small)\n",
    "offset += len(df_small)\n"
   ],
   "id": "cb7d9232f5aa9fb5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Аналитика переписки (треды, темы, теги проектов)",
   "id": "e896baf89c2a5f9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:31.516018Z",
     "start_time": "2025-12-01T21:25:31.339607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "# Утилиты\n",
    "def split_addrs(x) -> list[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [a.strip() for a in x if str(a).strip()]\n",
    "    return [a.strip() for a in re.split(r'[;,]', str(x)) if a.strip()]\n",
    "\n",
    "def participants(row) -> list[str]:\n",
    "    lst = split_addrs(row.get(\"from_addr\")) + split_addrs(row.get(\"to_addr\")) \\\n",
    "        + split_addrs(row.get(\"cc_addr\")) + split_addrs(row.get(\"bcc_addr\"))\n",
    "    return sorted(set(lst))\n",
    "\n",
    "def clean_for_topics(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    t = text.lower()\n",
    "    t = re.sub(r'[^a-zа-я0-9\\s\\-]+', ' ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    toks = t.split()\n",
    "    stop = set(\"\"\"\n",
    "        и в во на с со от до по за для при как что это это/that of the a an to is are was were be been being\n",
    "        у о об обo про из из-за над под между но или либо либо/или который которые какая какие чей чья чей-то\n",
    "        re fw fwd subject тема письмо письма письме по- поводу\n",
    "    \"\"\".split())\n",
    "    return [w for w in toks if len(w) > 2 and w not in stop]\n",
    "\n",
    "PROJECT_KEYWORDS = {\n",
    "    \"segezha\": \"Проект: Segezha\",\n",
    "    \"ewm\": \"SAP EWM\",\n",
    "    \"bw\": \"SAP BW\",\n",
    "    \"o2c\": \"O2C\",\n",
    "    \"mnf\": \"MNF\",\n",
    "    \"сцбк\": \"СЦБК\",\n",
    "    \"вагон\": \"Логистика/Вагоны\",\n",
    "}\n",
    "\n",
    "def guess_project_tags(subject: str, body: str, addrs: list[str]) -> list[str]:\n",
    "    text = f\"{subject or ''} {body or ''}\".lower()\n",
    "    tags = set()\n",
    "    for kw, tag in PROJECT_KEYWORDS.items():\n",
    "        if kw in text:\n",
    "            tags.add(tag)\n",
    "    for a in addrs:\n",
    "        m = re.search(r'@([a-z0-9\\.-]+)', a.lower())\n",
    "        if m:\n",
    "            dom = m.group(1)\n",
    "            if 'segezha' in dom:\n",
    "                tags.add(\"Проект: Segezha\")\n",
    "            if 'bearingpoint' in dom:\n",
    "                tags.add(\"Внутренние/Подрядчик\")\n",
    "    return sorted(tags)\n",
    "\n",
    "# Добавление колонок в df\n",
    "df = emails_df.copy()\n",
    "df[\"norm_subject\"] = df[\"subject\"].apply(lambda s: normalize_subject(s if isinstance(s, str) else \"\"))\n",
    "df[\"participants\"] = df.apply(participants, axis=1)\n",
    "df[\"thread_key\"] = df.apply(lambda r: f'{r[\"norm_subject\"]}||{\";\".join(r[\"participants\"])}', axis=1)\n",
    "\n",
    "def plain_body(row):\n",
    "    t = (row.get(\"body_text\") or \"\").strip()\n",
    "    if t:\n",
    "        return t\n",
    "    return (row.get(\"body_html\") or \"\").strip()\n",
    "\n",
    "df[\"plain_body\"] = df.apply(plain_body, axis=1)\n",
    "df[\"topics\"] = df.apply(lambda r: clean_for_topics(r[\"subject\"]) + clean_for_topics(r[\"plain_body\"][:1000]), axis=1)\n",
    "df[\"project_tags\"] = df.apply(lambda r: guess_project_tags(r[\"subject\"], r[\"plain_body\"], r[\"participants\"]), axis=1)\n",
    "\n",
    "threads = (\n",
    "    df.groupby(\"thread_key\")\n",
    "      .agg(\n",
    "         first_sent=(\"sent_at_utc\",\"min\"),\n",
    "         last_sent=(\"sent_at_utc\",\"max\"),\n",
    "         n_emails=(\"id\",\"count\"),\n",
    "         subjects=(\"subject\", lambda s: list(pd.unique([x for x in s if isinstance(x, str)]))[:5]),\n",
    "         participants=(\"participants\", lambda cols: sorted(set(sum(cols, [])))),\n",
    "         projects=(\"project_tags\", lambda cols: sorted(set(sum(cols, [])))),\n",
    "         topics=(\"topics\", lambda cols: [w for w,_ in Counter(sum(cols, [])).most_common(10)])\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"last_sent\",\"n_emails\"], ascending=[False, False])\n",
    ")\n",
    "\n",
    "def flatten(col):\n",
    "    out = []\n",
    "    for arr in col:\n",
    "        out.extend(arr)\n",
    "    return out\n",
    "\n",
    "projects_df = (\n",
    "    threads.assign(project=lambda x: x[\"projects\"].apply(lambda arr: arr if arr else [\"(Не классифицировано)\"]))\n",
    "           .explode(\"project\")\n",
    "           .groupby(\"project\")\n",
    "           .agg(\n",
    "              n_threads=(\"thread_key\",\"nunique\"),\n",
    "              n_emails=(\"n_emails\",\"sum\"),\n",
    "              first_sent=(\"first_sent\",\"min\"),\n",
    "              last_sent=(\"last_sent\",\"max\"),\n",
    "              participants=(\"participants\", lambda cols: sorted(set(flatten(cols)))[:50]),\n",
    "              top_topics=(\"topics\", lambda cols: [w for w,_ in Counter(flatten(cols)).most_common(15)])\n",
    "           )\n",
    "           .reset_index()\n",
    "           .sort_values([\"n_threads\",\"n_emails\"], ascending=[False, False])\n",
    ")\n"
   ],
   "id": "35891f0e3f18d5d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_158240\\3129374694.py:77: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  subjects=(\"subject\", lambda s: list(pd.unique([x for x in s if isinstance(x, str)]))[:5]),\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сводка тредов с помощью LLM",
   "id": "ca1e88bce26e6647"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:31.671306Z",
     "start_time": "2025-12-01T21:25:31.530023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_project_emails(project_hint: str, limit: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Найти и сгруппировать письма по проекту/теме.\n",
    "\n",
    "    Вход:\n",
    "        project_hint: текстовая подсказка — название проекта, код ЗНИ, ключевые слова.\n",
    "        limit: максимум писем для выборки.\n",
    "\n",
    "    Логика:\n",
    "      1) делаем семантический поиск по Qdrant;\n",
    "      2) группируем результаты по thread_key;\n",
    "      3) сортируем письма внутри треда по sent_at_utc.\n",
    "\n",
    "    Выход:\n",
    "        JSON-строка со структурой:\n",
    "        {\n",
    "          \"project_hint\": \"...\",\n",
    "          \"threads\": [\n",
    "            {\n",
    "              \"thread_key\": \"...\",\n",
    "              \"subject\": \"...\",\n",
    "              \"participants\": [...],\n",
    "              \"messages\": [\n",
    "                {\n",
    "                  \"message_id\": \"...\",\n",
    "                  \"sent_at_utc\": \"...\",\n",
    "                  \"from_addr\": \"...\",\n",
    "                  \"snippet\": \"...\",\n",
    "                  \"subject\": \"...\",\n",
    "                  \"folder\": \"...\",\n",
    "                  \"participants\": [...],\n",
    "                  \"metadata\": {...}\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # 1. Семантический поиск по тексту\n",
    "    docs = qv.similarity_search(project_hint, k=limit)\n",
    "\n",
    "    threads = defaultdict(list)\n",
    "\n",
    "    for doc in docs:\n",
    "        md = doc.metadata or {}\n",
    "\n",
    "        thread_key = md.get(\"thread_key\") or md.get(\"subject\") or \"unknown_thread\"\n",
    "\n",
    "        message = {\n",
    "            \"row_id\": md.get(\"row_id\"),\n",
    "            \"message_id\": md.get(\"message_id\"),\n",
    "            \"sent_at_utc\": md.get(\"sent_at_utc\"),\n",
    "            \"from_addr\": (md.get(\"from_addr\") or [None])[0],\n",
    "            \"subject\": md.get(\"subject\"),\n",
    "            \"folder\": md.get(\"folder\"),\n",
    "            \"participants\": md.get(\"participants\") or [],\n",
    "            # обрезаем тело, чтобы не заливать в ответ мегатекст\n",
    "            \"snippet\": (doc.page_content[:600] + \"…\") if doc.page_content else None,\n",
    "            \"metadata\": md,\n",
    "        }\n",
    "\n",
    "        threads[thread_key].append(message)\n",
    "\n",
    "    thread_list = []\n",
    "    for thread_key, messages in threads.items():\n",
    "        # сортируем по дате\n",
    "        messages_sorted = sorted(\n",
    "            messages,\n",
    "            key=lambda m: m.get(\"sent_at_utc\") or \"\",\n",
    "        )\n",
    "\n",
    "        # subject — из какого-то письма (обычно это RE:/FW: и т.п.)\n",
    "        subject = None\n",
    "        for m in messages_sorted:\n",
    "            if m.get(\"subject\"):\n",
    "                subject = m[\"subject\"]\n",
    "                break\n",
    "\n",
    "        # участники: агрегируем по всем письмам\n",
    "        participants_set = set()\n",
    "        for m in messages_sorted:\n",
    "            for p in m.get(\"participants\") or []:\n",
    "                participants_set.add(p)\n",
    "\n",
    "        thread_list.append(\n",
    "            {\n",
    "                \"thread_key\": thread_key,\n",
    "                \"subject\": subject,\n",
    "                \"participants\": sorted(participants_set),\n",
    "                \"messages\": messages_sorted,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    result = {\n",
    "        \"project_hint\": project_hint,\n",
    "        \"threads\": thread_list,\n",
    "    }\n",
    "\n",
    "    return json.dumps(result, ensure_ascii=False, indent=2)\n"
   ],
   "id": "7356135747b1e76b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:31.703309Z",
     "start_time": "2025-12-01T21:25:31.680308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tool\n",
    "def search_emails_raw(query: str, limit: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Общий поиск писем в Qdrant по любому запросу (без группировки).\n",
    "    Удобно для отладки.\n",
    "    \"\"\"\n",
    "    docs = qv.similarity_search(query, k=limit)\n",
    "    data = []\n",
    "    for doc in docs:\n",
    "        md = doc.metadata or {}\n",
    "        data.append({\n",
    "            \"thread_key\": md.get(\"thread_key\"),\n",
    "            \"row_id\": md.get(\"row_id\"),\n",
    "            \"message_id\": md.get(\"message_id\"),\n",
    "            \"subject\": md.get(\"subject\"),\n",
    "            \"sent_at_utc\": md.get(\"sent_at_utc\"),\n",
    "            \"from_addr\": (md.get(\"from_addr\") or [None])[0],\n",
    "            \"participants\": md.get(\"participants\") or [],\n",
    "            \"folder\": md.get(\"folder\"),\n",
    "            \"snippet\": (doc.page_content[:400] + \"…\") if doc.page_content else None,\n",
    "        })\n",
    "    return json.dumps({\"query\": query, \"results\": data}, ensure_ascii=False, indent=2)"
   ],
   "id": "b1f742bb3b210c06",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:25:31.859467Z",
     "start_time": "2025-12-01T21:25:31.711307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    PIIMiddleware,\n",
    "    SummarizationMiddleware,\n",
    ")\n",
    "# from langchain_tavily import TavilySearch  # если понадобится веб-поиск\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Ты — аналитик переписки по проектам.\n",
    "\n",
    "Твоя задача: по запросу пользователя сделать отчёт по проекту,\n",
    "используя письма из базы (Qdrant).\n",
    "\n",
    "Алгоритм действий:\n",
    "1) Понять по формулировке пользователя, какой проект/тема интересует\n",
    "   (например, компания, ЗНИ, код проекта, ключевые слова).\n",
    "2) Вызвать инструмент `search_project_emails` с осмысленным project_hint:\n",
    "   - в hint можно использовать название проекта,\n",
    "     ключевые слова (например, \"Segezha\", \"ЗНИ 239\", \"номер вагона\" и т.п.).\n",
    "3) Из результата инструмента:\n",
    "   - просмотреть список `threads`;\n",
    "   - для каждого треда понять:\n",
    "       * о чём тред (по subject, snippet'ам писем),\n",
    "       * какие решения были приняты,\n",
    "       * какие действия согласованы,\n",
    "       * есть ли открытые вопросы/риски,\n",
    "       * кто основные участники.\n",
    "4) Сформировать отчёт для пользователя:\n",
    "\n",
    "Формат ответа:\n",
    "1. Краткое резюме по проекту (2–10 абзацев):\n",
    "   - основная тема обсуждений,\n",
    "   - ключевые решения и договорённости,\n",
    "   - текущий статус (по возможности).\n",
    "2. Таблица или маркированный список по тредам:\n",
    "   - Название/subject треда,\n",
    "   - основная тема,\n",
    "   - краткое содержание (1–3 пункта),\n",
    "   - итог/статус.\n",
    "3. Если информации недостаточно или она шумная — обязательно упомяни это.\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5\",  # или твой модельный endpoint\n",
    "    tools=[\n",
    "        search_project_emails,\n",
    "        search_emails_raw,   # опционально\n",
    "    ],\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    middleware=[\n",
    "        # скрываем e-mail и телефоны в вводе/выводе (настраиваем по желанию)\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
    "        PIIMiddleware(\n",
    "            \"phone_number\",\n",
    "            detector=(\n",
    "                r\"(?:\\+?\\d{1,3}[\\s.-]?)?\"\n",
    "                r\"(?:\\(?\\d{2,4}\\)?[\\s.-]?)?\"\n",
    "                r\"\\d{3,4}[\\s.-]?\\d{4}\"\n",
    "            ),\n",
    "            strategy=\"redact\",\n",
    "        ),\n",
    "        # если переписка большая, middleware будет автоматически её подрезать и резюмировать\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-5\",\n",
    "            max_tokens_before_summary=500,\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ],
   "id": "33734fa832c19346",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:27:34.802534Z",
     "start_time": "2025-12-01T21:25:31.872013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Сделай отчёт по проекту Segezha на основе переписки. Какие темы поднимались в переписках, для каждой темы краткое резюме к чему пришли в итоге\n",
    "\n",
    "            Если идет переписка по локальному вопросу, например по конкретному Бизнес-партнеру, конкретному материалу, что какой-то документ не выгрузился, или какой-то конкретный документ надо скорректировать и т.д., то нужно очень верхнеуровнево одной строкой отметить суть, не перечисляя каждый такой инцидент. Если таких локальных вопросов повторяется определенное количество, то стоит отметить повторяемость таких вопросов/инцидентов \"\"\"\n",
    "\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "report = result[\"messages\"][-1].content\n",
    "print(report)\n"
   ],
   "id": "c73304b557cb02e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчет по проекту Segezha (по переписке)\n",
      "\n",
      "Краткое резюме\n",
      "- Основные темы периода: сверка и исправление внутригрупповых оборотов (ВГО) и валютных расхождений; интеграция TM–SD для корректного расчета выручки (ЗНИ 272); отражение и распределение фактических транспортных затрат и начислений; утверждение регламентов по TM; ряд эксплуатационных инцидентов (интерфейсы с Transporeon, НСИ/БП, корректировки печатных форм и ролей).\n",
      "- ВГО/валюты: согласован порядок работы с расхождениями, подготовлено и согласовано ЗНИ 281; определены сроки корректировок (при согласовании – до 5 марта). По «Зеркалу 2» внесены правки (двойное налогообложение, кодировки валют), но остались вопросы по кодам валют (EUR vs UEU) и работе зеркала при нескольких ДРФ в одном документе.\n",
      "- TM–SD (ЗНИ 272): согласовано решение по передаче даты коносамента из TM и заполнению даты перехода рисков (FOB/CFR/CIF) в исходящей поставке. Заказчик логистика, требуется расширение объема ЗНИ (контроли в SD); ориентировочный срок продуктивного переноса — не ранее середины марта.\n",
      "- Транспортные затраты: определены требования к распределению фактической стоимости перевозки по позициям сбытовых заказов (пропорционально количеству/объему); инициирована оценка доработки ТМ‑27 для множественной контировки на несколько ОУР (тест/постановка ~5 ч/д; разработка — в оценке). Отдельно предложен временный процесс ежемесячных начислений на базе плановых ставок маршрутов с последующим сторно в следующем периоде (ограничение: АРКА).\n",
      "- Регламенты TM: выложены на согласование до 01.03; комментарии ИТ учтены, автором внесены правки; цикл согласования продолжается.\n",
      "- Эксплуатация/поддержка: ряд локальных инцидентов (выгрузки из Transporeon, корректировки экспедитора, создание БП для склада, доступы/роли, печатные формы) оперативно закрыты через SAPSP/Jira; часть случаев требует подтверждения сроков включения в продуктив или уточнения постановки задач.\n",
      "- Ключевые участники: со стороны Segezha — А. Сафонов, О. Спесивова, Е. Волкова, О. Емельянова, И. Наливко, А. Юдин, Е. Щукин, Д. Хусаинов и др.; со стороны консультационной команды — П. Острик, И. Жилин, И. Дондуков, А. Иваненко, А. Федосеев, А. Матерков, О. Нестеров и др.\n",
      "\n",
      "Темы и треды\n",
      "\n",
      "1) Тема: ВГО — расхождения и процесс урегулирования\n",
      "- Тред: “RE: расхождения по ВГО”\n",
      "- Суть: расхождения между парами ДЗО (в т.ч. по курсовым разницам, пара УК СГ–ВФК; влияние документов в UEU вместо RUB; связывание счетов по разным валютам).\n",
      "- Итоги/решения:\n",
      "  - Согласован регламент обработки расхождений: МСФО→Емельянова→ведущий бухгалтер→оперативный анализ/ответ.\n",
      "  - Подготовлено ЗНИ (указано ЗНИ 281 — согласовано); трудозатраты на корректировки — 3 ч/д; целевой срок выполнения — при согласовании до 5 марта.\n",
      "  - Задано требование к единообразию валют в связанных документах; отмечена проблема документов, проведенных в UEU.\n",
      "- Статус: в работе по плану, часть корректировок запланирована к 05.03; регламент коммуникаций установлен.\n",
      "\n",
      "2) Тема: «Зеркало 2» — валютные счета и кодировки\n",
      "- Треды: “RE: Зеркало 2: мемо — для информации по валютным счетам” (две переписки)\n",
      "- Суть: исправление зеркалирования (двойное налогообложение), добавление кодировок валют (EUR→UEU, USD→UUS), вопросы по единству валют EUR–EUR vs EUR–UEU; зеркала не создаются при нескольких ДРФ в одном документе.\n",
      "- Итоги/решения:\n",
      "  - Внесены правки и выложены материалы/ссылка.\n",
      "  - Подтвержден принцип «валюта передается 1‑в‑1», нужен однозначный выбор кода.\n",
      "  - Рекомендовано делить документы по ДРФ; проверена работа при одном ДРФ — зеркало создается.\n",
      "- Статус: частично реализовано; открыто: поддержка нескольких ДРФ/документ, утверждение кодов валют.\n",
      "\n",
      "3) Тема: Интеграция TM–SD для расчета выручки (ЗНИ 272)\n",
      "- Треды: “RE: Корректность данных в SAP для расчета выручки (… ЗНИ 272)” (несколько переписок)\n",
      "- Суть: передача из TM даты коносамента и автоматическое заполнение даты перехода рисков в исходящей поставке (FOB/CFR/CIF) для корректной выручки.\n",
      "- Итоги/решения:\n",
      "  - ЗНИ 272 согласован; нужно доработать спеку (часть SD), добавить контроли в заказы.\n",
      "  - Ориентир по срокам — не ранее середины марта; анонс 1–2 мес. на реализацию.\n",
      "  - Коммуникация клиенту о плановом сроке переноса в продуктив — запрошена.\n",
      "- Статус: согласовано, дополняется ТЗ/объем; целевые сроки — TBC (mid‑March earliest).\n",
      "\n",
      "4) Тема: Фактические транспортные затраты — распределение и разводка на ОУР (ТМ‑27)\n",
      "- Треды: “RE: Разнесение фактических транспортных затрат” (несколько), инвайты на встречи\n",
      "- Суть: распределять сумму перевозки по позициям СО пропорционально количеству/объему; обеспечить множественную контировку (несколько ОУР) при интеграции TM→S/4; синхронизация цепочки документов в S/4.\n",
      "- Итоги/решения:\n",
      "  - Требования зафиксированы; подтверждение, что все данные уже приходят в входящем XML; обозначен стек классов обработки в S/4.\n",
      "  - Оценка: тест/постановка ~5 ч/д; доработка ТМ‑27 (множественная контировка) — оценка со стороны разработки запрошена (ответственный — П. Острик).\n",
      "- Статус: в оценке/планировании; технический путь и границы — определены.\n",
      "\n",
      "5) Тема: Ежемесячные начисления транспортных расходов\n",
      "- Тред: “RE: еще раз про логистику”\n",
      "- Суть: временный процесс начислений для полноты затрат периода и «выручки очищенной от транспорта».\n",
      "- Итоги/решения:\n",
      "  - Предложен процесс: отчет → сворачивание по аналитикам (экспорт/вн. рынок; ВГО/сторонние) → начисления в FI → сторно в следующем периоде; ставки — плановые из маршрутов; ограничение — АРКА.\n",
      "  - Запрошен апдейт по действиям/решениям.\n",
      "- Статус: предложено, требуется организационное внедрение/подтверждение.\n",
      "\n",
      "6) Тема: Регламенты TM (включая ОП)\n",
      "- Треды: “RE: Регламенты группа ТМ”, “RE: Регламент ОП группа ТМ”\n",
      "- Суть: выложены регламенты в СЭД на согласование до 01.03; получены доп. комментарии ИТ; автором (О. Нестеров) исправления внесены.\n",
      "- Статус: согласование в СЭД до 01.03; итерации правок продолжаются.\n",
      "\n",
      "7) Тема: ЗНИ 239 — изменение номера вагона\n",
      "- Треды: “RE: ЗНИ 239 Изменение номера вагона”\n",
      "- Суть: в уже реализованных печатных формах заменить источник данных поля «номер вагона»; параллельно — вопросы постановки задачи по паспорту качества/BW.\n",
      "- Итоги/решения: требование по печатным формам уточнено; запрошен корректный документ‑постановка; в BW — поле должно появиться в ключевых отчетах.\n",
      "- Статус: требуется финализация постановки и согласование объема работ.\n",
      "\n",
      "8) Тема: ЗНИ‑282 — изменение схемы учета вывозки сырья\n",
      "- Треды: “RE: Обсуждение вопросов по ЗНИ‑282 …” (в т.ч. фwd приглашения/вложения)\n",
      "- Суть: обсуждение документа ЗНИ‑282; назначены обсуждения/встречи; согласование состава работ.\n",
      "- Статус: в проработке (на стадии встреч и согласования).\n",
      "\n",
      "9) Тема: Автораспределение поставок по ж/д (SAPSP‑17498)\n",
      "- Тред: “SAPSP‑17498 автоматическое распределение поставок по ж.д.”\n",
      "- Суть: запрос на включение автоматического распределения; согласование необходимости.\n",
      "- Итоги/решения: “Да, сделать”; запрос на уведомление даты запуска.\n",
      "- Статус: ожидается подтверждение даты включения в продуктив.\n",
      "\n",
      "10) Координация/ролей\n",
      "- Тред: “RE: Склад и Транспорт в Сегежа и Полюс”\n",
      "- Суть: закреплены контактные лица: по Сегежа — П. Острик; по Полюс — С. (указано в письме).\n",
      "- Статус: роли распределены.\n",
      "\n",
      "Локальные вопросы/инциденты (повторяющиеся типы, кратко)\n",
      "- Transporeon → S/4: “не выгрузился номер авто 6100065014” (SAPSP‑18530) — причина: зависшая очередь сериализации; оперативно устранено (статус «Готово»). Параллельно клиент временно внес данные вручную. Подобные инциденты повторяются (единичные случаи).\n",
      "- Корректировка экспедитора по поставке 6100049111 (SAPSP‑18554) — выполнено, «Готово».\n",
      "- Создание/расширение БП для склада (S015, SEP500) — запрос НСИ для корректной интеграции ERP→TM (единичные случаи).\n",
      "- Доступы/роли (SAPSP‑18265 «Открыть доступ к поставкам Лесосибирского ЛДК») — открытые вопросы по формулировке роли/настройке профиля (требуется подтверждение готовности).\n",
      "- Ж/Д печать ГУ‑29 (контейнеры, СЦБК) — запрошены сроки запуска печати полного комплекта (локальная операционная тема).\n",
      "- Актуализация Приложения 1 к ТЗ (TM.021 и пр.) — запрос на предоставление актуальных версий (документооборот).\n",
      "\n",
      "Замечания по полноте и шуму\n",
      "- Часть переписки носит уведомительный/календарный характер (инвайты, подписи), без результатов — информация шумная.\n",
      "- По ряду тем не зафиксированы явные финальные решения/даты (ЗНИ‑239 — требуется финализация постановки; SAPSP‑17498 — ожидается дата запуска; SAPSP‑18265 — требуется подтверждение настройки профиля).\n",
      "- По «Зеркалу 2» вопросов с несколькими ДРФ в документе — обсуждение есть, окончательного решения в письмах не зафиксировано.\n",
      "\n",
      "Если нужна детализация по конкретному ЗНИ/тикету (ссылки, участники, даты) — уточните, предоставлю развернутую выжимку по выбранной теме.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### вывод инфо",
   "id": "d25166e7fb44000f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:27:35.177301Z",
     "start_time": "2025-12-01T21:27:35.168302Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2df566e78a4d3596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:27:35.192302Z",
     "start_time": "2025-12-01T21:27:35.184302Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "369eceddc2d7617",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
